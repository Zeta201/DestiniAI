{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQOWhSnI_Mnn",
        "outputId": "81ce3c99-46fc-46dc-9445-d1589391e467"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "Collecting numpy<2\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall numpy -y\n",
        "!pip install \"numpy<2\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torch torchvision torchaudio -y\n",
        "!pip install torch==2.5.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tKlkhNc5KYk",
        "outputId": "b8a04f1f-1513-4941-f7a4-98282f595b1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.6.0+cu124\n",
            "Uninstalling torch-2.6.0+cu124:\n",
            "  Successfully uninstalled torch-2.6.0+cu124\n",
            "Found existing installation: torchvision 0.21.0+cu124\n",
            "Uninstalling torchvision-0.21.0+cu124:\n",
            "  Successfully uninstalled torchvision-0.21.0+cu124\n",
            "Found existing installation: torchaudio 2.6.0+cu124\n",
            "Uninstalling torchaudio-2.6.0+cu124:\n",
            "  Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch==2.5.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.0%2Bcu121-cp311-cp311-linux_x86_64.whl (780.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.5/780.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.5.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.5.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.5.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.5.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.5.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.5.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.5.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.5.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0) (2.21.5)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.5.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.1.0 (from torch==2.5.0)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.5.0) (12.5.82)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.0%2Bcu121-cp311-cp311-linux_x86_64.whl (7.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.5.0%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.5.0) (3.0.2)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nvtx-cu12-12.1.105 torch-2.5.0+cu121 torchaudio-2.5.0+cu121 torchvision-0.20.0+cu121 triton-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install surprise faker\n",
        "!pip install torch_geometric\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-2.5.0+cu121.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqx6RxeG_Otn",
        "outputId": "00dab898-0118-43d2-d5a5-f1d49d5c6864"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting surprise\n",
            "  Downloading surprise-0.1-py2.py3-none-any.whl.metadata (327 bytes)\n",
            "Collecting faker\n",
            "  Downloading faker-37.1.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting scikit-surprise (from surprise)\n",
            "  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from faker) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-surprise->surprise) (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-surprise->surprise) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-surprise->surprise) (1.14.1)\n",
            "Downloading surprise-0.1-py2.py3-none-any.whl (1.8 kB)\n",
            "Downloading faker-37.1.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp311-cp311-linux_x86_64.whl size=2505210 sha256=164a7bcf460e2c6566162a18a37053ab58b4bf3bd327ce0c963d7576af7165c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/8f/6e/7e2899163e2d85d8266daab4aa1cdabec7a6c56f83c015b5af\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: faker, scikit-surprise, surprise\n",
            "Successfully installed faker-37.1.0 scikit-surprise-1.1.4 surprise-0.1\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.5.0+cu121.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu121/torch_sparse-0.6.18%2Bpt25cu121-cp311-cp311-linux_x86_64.whl (5.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.14.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse) (1.26.4)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt25cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "print(\"CUDA version:\", torch.version.cuda)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2WdS4N03W_u",
        "outputId": "9fbafc22-2e5d-42c6-db41-8a226837782b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version: 2.5.0+cu121\n",
            "CUDA version: 12.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "from faker import Faker\n",
        "\n",
        "# Initialize Faker\n",
        "fake = Faker()\n",
        "\n",
        "# Predefined list of locations with fixed latitude and longitude\n",
        "locations_dict = {\n",
        "    \"New York\": (40.712776, -74.005974),\n",
        "    \"Los Angeles\": (34.052235, -118.243683),\n",
        "    \"Chicago\": (41.878113, -87.629799),\n",
        "    \"Houston\": (29.760427, -95.369804),\n",
        "    \"Phoenix\": (33.448376, -112.074036),\n",
        "    \"Philadelphia\": (39.952583, -75.165222),\n",
        "    \"San Antonio\": (29.424122, -98.493629),\n",
        "    \"San Diego\": (32.715736, -117.161087),\n",
        "    \"Dallas\": (32.776665, -96.796989),\n",
        "    \"San Jose\": (37.338208, -121.886329),\n",
        "    \"Austin\": (30.267153, -97.743061),\n",
        "    \"Seattle\": (47.606209, -122.332069),\n",
        "    \"Denver\": (39.739236, -104.990251),\n",
        "    \"Boston\": (42.360081, -71.058884),\n",
        "    \"Atlanta\": (33.749001, -84.387978),\n",
        "    \"Miami\": (25.761680, -80.191790),\n",
        "    \"Orlando\": (28.538336, -81.379234),\n",
        "    \"Portland\": (45.515232, -122.678448),\n",
        "    \"Minneapolis\": (44.977753, -93.265015),\n",
        "    \"Tampa\": (27.950575, -82.457177),\n",
        "    \"Indianapolis\": (39.768403, -86.158068),\n",
        "    \"Cleveland\": (41.499320, -81.694361),\n",
        "    \"Detroit\": (42.331427, -83.045753),\n",
        "    \"Las Vegas\": (36.169941, -115.139832),\n",
        "    \"Sacramento\": (38.581572, -121.494400),\n",
        "    \"Kansas City\": (39.099727, -94.578568),\n",
        "    \"Salt Lake City\": (40.760779, -111.891047),\n",
        "    \"Columbus\": (39.961176, -82.998794),\n",
        "    \"Raleigh\": (35.779591, -78.638176)\n",
        "}\n",
        "\n",
        "\n",
        "# Generate random users with consistent locations\n",
        "def generate_users(n=100):\n",
        "    start_date = pd.to_datetime(\"2023-01-01\")  # Earliest possible signup date\n",
        "    end_date = pd.to_datetime(\"2024-04-01\")  # Latest possible signup date\n",
        "\n",
        "    # Sample locations from the predefined list\n",
        "    location_choices = list(locations_dict.keys())\n",
        "\n",
        "    data = {\n",
        "        'user_id': list(range(1, n + 1)),\n",
        "        'name': [fake.name() for _ in range(n)],\n",
        "        'age': [random.randint(18, 50) for _ in range(n)],\n",
        "        'gender': [random.choice(['M', 'F', 'Non-binary']) for _ in range(n)],\n",
        "        'location': [random.choice(location_choices) for _ in range(n)],\n",
        "        'latitude': [locations_dict[loc][0] for loc in random.choices(location_choices, k=n)],  # Fixed latitude based on location\n",
        "        'longitude': [locations_dict[loc][1] for loc in random.choices(location_choices, k=n)],  # Fixed longitude based on location\n",
        "        'interests': [', '.join(random.sample(\n",
        "            ['music', 'travel', 'sports', 'technology', 'art', 'reading', 'gaming', 'photography', 'cooking', 'fitness'], 3)) for _ in range(n)],\n",
        "        'about_me': [fake.sentence(nb_words=10) for _ in range(n)],\n",
        "        'relationship_goal': [random.choice(['Casual Dating', 'Long-term', 'Friends', 'Networking']) for _ in range(n)],\n",
        "        'personality': [random.choice(['Introvert', 'Extrovert', 'Ambivert']) for _ in range(n)],\n",
        "        'MBTI': [random.choice(['INFJ', 'ENTP', 'ISTJ', 'ENFP', 'ISFJ', 'ESTP', 'INTP', 'ESFJ']) for _ in range(n)],\n",
        "        'likes': [random.sample(range(1, n + 1), random.randint(5, 15)) for _ in range(n)],\n",
        "        'swipes': [random.randint(10, 500) for _ in range(n)],\n",
        "        'messages_sent': [random.randint(0, 100) for _ in range(n)],\n",
        "        'response_rate': [round(random.uniform(0.2, 1.0), 2) for _ in range(n)],\n",
        "        'signup_date': [fake.date_between(start_date=start_date, end_date=end_date) for _ in range(n)],\n",
        "        'education_level': [random.choice(['High School', 'Bachelor’s', 'Master’s', 'PhD']) for _ in range(n)],\n",
        "        'occupation': [fake.job() for _ in range(n)],\n",
        "        'zodiac_sign': [random.choice(['Aries', 'Taurus', 'Gemini', 'Cancer', 'Leo', 'Virgo', 'Libra',\n",
        "                                       'Scorpio', 'Sagittarius', 'Capricorn', 'Aquarius', 'Pisces']) for _ in range(n)],\n",
        "        'height_cm': [random.randint(150, 200) for _ in range(n)],\n",
        "        'body_type': [random.choice(['Athletic', 'Average', 'Curvy', 'Slim']) for _ in range(n)],\n",
        "        'languages_spoken': [', '.join(random.sample(['English', 'Spanish', 'French', 'Mandarin', 'Hindi'], k=random.randint(1, 3))) for _ in range(n)],\n",
        "        'dating_preference': [random.choice(['Men', 'Women', 'Everyone']) for _ in range(n)],\n",
        "        'likes_received': [random.randint(0, 100) for _ in range(n)],\n",
        "        'photo_count': [random.randint(1, 6) for _ in range(n)],\n",
        "        'is_verified': [random.choice([True, False]) for _ in range(n)],\n",
        "        'daily_active_minutes': [random.randint(0, 120) for _ in range(n)],\n",
        "        'last_login': [fake.date_between(start_date=pd.to_datetime(\"2024-03-01\"), end_date=pd.to_datetime(\"2024-04-01\")) for _ in range(n)],\n",
        "        'is_premium_user': [random.choices([True, False], weights=[0.2, 0.8])[0] for _ in range(n)]\n",
        "    }\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Generate 1000 users as an example\n",
        "df_users = generate_users(n=1000)\n",
        "df_users.head(1)\n",
        "df_users.to_csv('users.csv', index=False)\n"
      ],
      "metadata": {
        "id": "5_MuC13C_nd1"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_users = generate_users(1000)"
      ],
      "metadata": {
        "id": "Iaa8WHF5Ckqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Content Based Filtering (CBF)"
      ],
      "metadata": {
        "id": "-o3sCdDLBG7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch_geometric\n",
        "from torch_geometric.nn import SAGEConv\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch_geometric.data import Data\n",
        "import random\n",
        "from scipy.spatial.distance import cdist\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "cyy42pf6_9p6"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up GPU device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load the pre-trained BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model_bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "# Load Sentence-BERT model for embedding\n",
        "sentence_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rI-PbaEz_8yn",
        "outputId": "01521204-9017-4b7d-fdbe-a09cffcbe622"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_interest_similarity(df_users, pairs):\n",
        "    interests1 = df_users.iloc[pairs[:, 0]]['interests'].values\n",
        "    interests2 = df_users.iloc[pairs[:, 1]]['interests'].values\n",
        "\n",
        "    # Encode interests with Sentence-BERT\n",
        "    embeddings1 = sentence_model.encode(interests1, convert_to_numpy=True)\n",
        "    embeddings2 = sentence_model.encode(interests2, convert_to_numpy=True)\n",
        "\n",
        "    # Compute cosine similarity between interest embeddings\n",
        "    interest_similarity = cosine_similarity(embeddings1, embeddings2)\n",
        "    return interest_similarity.flatten()\n",
        "\n",
        "# Function to generate BERT embeddings for a list of sentences\n",
        "def get_bert_embeddings(sentences):\n",
        "    # Tokenize the sentences\n",
        "    inputs = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    # Get the embeddings from BERT model\n",
        "    with torch.no_grad():\n",
        "        outputs = model_bert(**inputs)\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1)  # Mean pooling\n",
        "    return embeddings\n",
        "\n",
        "# Function to calculate haversine distance (in kilometers)\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    R = 6371\n",
        "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
        "    dlat = lat2 - lat1\n",
        "    dlon = lon2 - lon1\n",
        "    a = np.sin(dlat / 2) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2) ** 2\n",
        "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
        "    return R * c"
      ],
      "metadata": {
        "id": "Rr1Kn0NuAGnn"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7RqjhmUVAGrG"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.spatial.distance import cdist\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Haversine distance function\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    R = 6371  # Earth radius in km\n",
        "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
        "    dlat = lat2 - lat1\n",
        "    dlon = lon2 - lon1\n",
        "    a = np.sin(dlat / 2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0)**2\n",
        "    c = 2 * np.arcsin(np.sqrt(a))\n",
        "    return R * c\n",
        "\n",
        "# Feature encoding and weighted fusion\n",
        "def build_feature_matrices(df_users):\n",
        "    for col in ['interests', 'about_me', 'personality', 'MBTI', 'relationship_goal',\n",
        "                'education_level', 'occupation', 'zodiac_sign']:\n",
        "        df_users[col] = df_users[col].fillna('').astype(str)\n",
        "\n",
        "    interests_emb = sentence_model.encode(df_users['interests'].tolist(), convert_to_numpy=True)\n",
        "    about_me_emb = sentence_model.encode(df_users['about_me'].tolist(), convert_to_numpy=True)\n",
        "    personality_emb = sentence_model.encode(df_users['personality'].tolist(), convert_to_numpy=True)\n",
        "    mbti_emb = sentence_model.encode(df_users['MBTI'].tolist(), convert_to_numpy=True)\n",
        "    relationship_emb = sentence_model.encode(df_users['relationship_goal'].tolist(), convert_to_numpy=True)\n",
        "    education_emb = sentence_model.encode(df_users['education_level'].tolist(), convert_to_numpy=True)\n",
        "    occupation_emb = sentence_model.encode(df_users['occupation'].tolist(), convert_to_numpy=True)\n",
        "    zodiac_emb = sentence_model.encode(df_users['zodiac_sign'].tolist(), convert_to_numpy=True)\n",
        "\n",
        "    combined = (\n",
        "        0.15 * interests_emb +\n",
        "        0.15 * about_me_emb +\n",
        "        0.10 * personality_emb +\n",
        "        0.10 * mbti_emb +\n",
        "        0.10 * relationship_emb +\n",
        "        0.10 * education_emb +\n",
        "        0.10 * occupation_emb +\n",
        "        0.20 * zodiac_emb\n",
        "    )\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    numerical_features = df_users[['age']].astype(float)\n",
        "    normalized_numerical = scaler.fit_transform(numerical_features)\n",
        "\n",
        "    return combined, normalized_numerical, scaler\n",
        "\n",
        "def build_graph(df_users, content_matrix, normalized_numerical, top_k=10):\n",
        "    num_users = len(df_users)\n",
        "    content_similarity = cosine_similarity(content_matrix)\n",
        "    num_similarity = 1 / (1 + cdist(normalized_numerical, normalized_numerical, metric='euclidean'))\n",
        "\n",
        "    latitudes = df_users['latitude'].values\n",
        "    longitudes = df_users['longitude'].values\n",
        "    lat1, lat2 = np.meshgrid(latitudes, latitudes)\n",
        "    lon1, lon2 = np.meshgrid(longitudes, longitudes)\n",
        "    distances = haversine(lat1, lon1, lat2, lon2)\n",
        "    location_similarity = np.exp(-distances / 100)\n",
        "\n",
        "    similarity_matrix = 0.33 * content_similarity + 0.33 * num_similarity + 0.33 * location_similarity\n",
        "\n",
        "    top_k_neighbors = np.argsort(-similarity_matrix, axis=1)[:, :top_k]\n",
        "    edge_list = [(i, j) for i in range(num_users) for j in top_k_neighbors[i] if i != j]\n",
        "    edge_index = torch.tensor(np.array(edge_list).T, dtype=torch.long).to(device)\n",
        "\n",
        "    node_features = np.hstack([content_matrix, normalized_numerical])\n",
        "    x = torch.tensor(node_features, dtype=torch.float).to(device)\n",
        "\n",
        "    return Data(x=x, edge_index=edge_index)\n",
        "\n",
        "def create_contrastive_pairs(df_users, batch_size=512, device='cpu'):\n",
        "    batch_size = min(batch_size, len(df_users))\n",
        "    indices = np.random.choice(len(df_users), (batch_size, 2), replace=True)\n",
        "    pairs = torch.tensor(indices, dtype=torch.long).to(device)\n",
        "\n",
        "    same_relationship = df_users.iloc[pairs[:, 0]]['relationship_goal'].values == df_users.iloc[pairs[:, 1]]['relationship_goal'].values\n",
        "    same_mbti = df_users.iloc[pairs[:, 0]]['MBTI'].values == df_users.iloc[pairs[:, 1]]['MBTI'].values\n",
        "\n",
        "    interests1 = df_users.iloc[pairs[:, 0]]['interests'].str.split(', ').apply(set)\n",
        "    interests2 = df_users.iloc[pairs[:, 1]]['interests'].str.split(', ').apply(set)\n",
        "    jaccard_sim = [(len(i1 & i2) / len(i1 | i2)) if len(i1 | i2) > 0 else 0 for i1, i2 in zip(interests1, interests2)]\n",
        "\n",
        "    age_diff = np.abs(df_users.iloc[pairs[:, 0]]['age'].values - df_users.iloc[pairs[:, 1]]['age'].values)\n",
        "    max_age = df_users['age'].max()\n",
        "    age_similarity = 1 - (age_diff / max_age) if max_age > 0 else np.zeros_like(age_diff)\n",
        "\n",
        "    lat1, lon1 = df_users.iloc[pairs[:, 0]]['latitude'].values, df_users.iloc[pairs[:, 0]]['longitude'].values\n",
        "    lat2, lon2 = df_users.iloc[pairs[:, 1]]['latitude'].values, df_users.iloc[pairs[:, 1]]['longitude'].values\n",
        "    distances = haversine(lat1, lon1, lat2, lon2)\n",
        "    location_similarity = np.exp(-distances / 100)\n",
        "\n",
        "    zodiac_similarity = (df_users.iloc[pairs[:, 0]]['zodiac_sign'].values == df_users.iloc[pairs[:, 1]]['zodiac_sign'].values)\n",
        "\n",
        "    labels = (0.1 * same_relationship.astype(float) +\n",
        "              0.05 * same_mbti.astype(float) +\n",
        "              0.1 * np.array(jaccard_sim) +\n",
        "              0.6 * age_similarity +\n",
        "              0.05 * zodiac_similarity.astype(float) +\n",
        "              0.1 * location_similarity)\n",
        "\n",
        "    labels = (labels > 0.5).astype(float)\n",
        "    return pairs.t(), torch.tensor(labels, dtype=torch.float).to(device)\n",
        "\n",
        "class GraphSAGE(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, 64)\n",
        "        self.conv2 = SAGEConv(64, out_channels)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "class ContrastiveLoss(torch.nn.Module):\n",
        "    def __init__(self, margin=0.5):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, output1, output2, label):\n",
        "        dist = F.pairwise_distance(output1, output2, p=2)\n",
        "        loss = 0.5 * (label * dist.pow(2) + (1 - label) * F.relu(self.margin - dist).pow(2))\n",
        "        return loss.mean()\n",
        "\n",
        "def train_gnn(data, df_users):\n",
        "    model = GraphSAGE(in_channels=data.x.shape[1], out_channels=64).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = ContrastiveLoss(margin=0.2).to(device)\n",
        "\n",
        "    for epoch in range(100):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        pair_indices, labels = create_contrastive_pairs(df_users, batch_size=512)\n",
        "        loss = criterion(out[pair_indices[0]], out[pair_indices[1]], labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def recommend_users_with_gnn(user_id, df_users, data, model, top_n=5):\n",
        "    user_idx = df_users[df_users['user_id'] == user_id].index[0]\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        user_embeddings = model(data).cpu().numpy()\n",
        "    user_embedding = user_embeddings[user_idx]\n",
        "    similarities = cosine_similarity([user_embedding], user_embeddings).flatten()\n",
        "    similar_users_idx = similarities.argsort()[-(top_n + 1):-1][::-1]\n",
        "    recommended_users = df_users.iloc[similar_users_idx].copy()\n",
        "    recommended_users[\"similarity_score\"] = similarities[similar_users_idx]\n",
        "    return recommended_users\n",
        "\n",
        "# Load the pre-trained BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model_bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Prepare feature matrices\n",
        "content_matrix, normalized_numerical, scaler = build_feature_matrices(df_users)\n",
        "data = build_graph(df_users, content_matrix, normalized_numerical)\n",
        "\n",
        "# Train GNN model on GPU\n",
        "model = train_gnn(data, df_users)\n",
        "\n",
        "# Get recommendations for a user\n",
        "user_id = 1\n",
        "print(f\"Recommendations for User {user_id}:\")\n",
        "recommended_users = recommend_users_with_gnn(user_id, df_users, data, model)\n",
        "print(recommended_users[['user_id', 'name', 'age', 'interests', 'personality', 'MBTI', 'relationship_goal', 'location', 'zodiac_sign', 'similarity_score']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmHw_wLPT4Iu",
        "outputId": "906fc1ef-3243-4f1d-e9d7-bc96b941b186"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.0089\n",
            "Epoch 10, Loss: 0.0030\n",
            "Epoch 20, Loss: 0.0026\n",
            "Epoch 30, Loss: 0.0027\n",
            "Epoch 40, Loss: 0.0027\n",
            "Epoch 50, Loss: 0.0026\n",
            "Epoch 60, Loss: 0.0024\n",
            "Epoch 70, Loss: 0.0024\n",
            "Epoch 80, Loss: 0.0023\n",
            "Epoch 90, Loss: 0.0025\n",
            "Recommendations for User 1:\n",
            "     user_id              name  age                      interests  \\\n",
            "907      908  Crystal Reynolds   47          gaming, travel, music   \n",
            "490      491   Melissa Barrett   47           cooking, art, sports   \n",
            "361      362      Stacey Smith   49  reading, fitness, photography   \n",
            "842      843      Jerry Newman   46   sports, photography, fitness   \n",
            "455      456   William Vasquez   48        travel, technology, art   \n",
            "\n",
            "    personality  MBTI relationship_goal     location zodiac_sign  \\\n",
            "907   Introvert  INFJ           Friends    Cleveland     Scorpio   \n",
            "490   Extrovert  ISFJ     Casual Dating      Phoenix      Cancer   \n",
            "361   Extrovert  ISTJ           Friends     New York      Taurus   \n",
            "842   Extrovert  ISFJ           Friends  Los Angeles     Scorpio   \n",
            "455   Introvert  INFJ         Long-term      Houston      Pisces   \n",
            "\n",
            "     similarity_score  \n",
            "907          0.999988  \n",
            "490          0.999956  \n",
            "361          0.999954  \n",
            "842          0.999946  \n",
            "455          0.999943  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_users[df_users['user_id']==1]['age']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "id": "nXOcynLaaANU",
        "outputId": "f8aee721-2a6f-4479-e032-7ac03310b2ef"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    47\n",
              "Name: age, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "\n",
        "# Define hyperparameter ranges\n",
        "margins = [0.1, 0.5, 1.0, 1.5, 2.0]\n",
        "hidden_dims = [32, 64, 128, 256]\n",
        "num_layers = [2, 3, 4]\n",
        "dropout_rates = [0.1, 0.3, 0.5]\n",
        "\n",
        "# Generate all combinations of hyperparameters\n",
        "hyperparameter_combinations = list(product(margins, hidden_dims, num_layers, dropout_rates))\n",
        "\n",
        "# Iterate over all combinations and train the model\n",
        "best_loss = float('inf')\n",
        "best_params = None\n",
        "\n",
        "for margin, hidden_dim, layers, dropout in hyperparameter_combinations:\n",
        "    print(f\"Training with margin={margin}, hidden_dim={hidden_dim}, layers={layers}, dropout={dropout}\")\n",
        "\n",
        "    # Adjust the GraphSAGE model based on the current combination\n",
        "    model = GraphSAGE(in_channels=data.x.shape[1], out_channels=hidden_dim, num_layers=layers, dropout_rate=dropout).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = ContrastiveLoss(margin=margin).to(device)\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(100):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "\n",
        "        pair_indices, labels = create_contrastive_pairs(df_users, batch_size=512)\n",
        "        output1 = out[pair_indices[0]]\n",
        "        output2 = out[pair_indices[1]]\n",
        "        loss = criterion(output1, output2, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
        "\n",
        "    # Check if this configuration gives the best performance\n",
        "    if loss.item() < best_loss:\n",
        "        best_loss = loss.item()\n",
        "        best_params = (margin, hidden_dim, layers, dropout)\n",
        "\n",
        "print(f\"Best Hyperparameters: {best_params} with Loss: {best_loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "b7s9ykYkBhS9",
        "outputId": "bee54458-efa3-4ac2-ca1d-fbb3c516a60c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with margin=0.1, hidden_dim=32, layers=2, dropout=0.1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "GraphSAGE.__init__() got an unexpected keyword argument 'num_layers'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-7ab19989b565>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Adjust the GraphSAGE model based on the current combination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGraphSAGE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mContrastiveLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmargin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmargin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: GraphSAGE.__init__() got an unexpected keyword argument 'num_layers'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Colaborative Filtering"
      ],
      "metadata": {
        "id": "TNwI8-iLBK7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise.model_selection import train_test_split\n",
        "from surprise import accuracy\n",
        "from surprise.model_selection import cross_validate\n",
        "\n",
        "def generate_interactions(n_users=1000, n_items=100):\n",
        "    interactions = []\n",
        "    for user in range(1, n_users + 1):\n",
        "        for _ in range(random.randint(5, 15)):  # Each user interacts with 5-15 profiles\n",
        "            item = random.randint(1, n_items)  # Random profile ID\n",
        "            rating = random.choice([1, 2, 3, 4, 5])  # Rating simulating a like or preference strength\n",
        "            interactions.append((user, item, rating))\n",
        "    return pd.DataFrame(interactions, columns=['user_id', 'profile_id', 'rating'])\n",
        "\n",
        "def build_collaborative_filtering_model(df):\n",
        "    \"\"\"Trains an SVD model on user-profile interactions.\"\"\"\n",
        "    reader = Reader(rating_scale=(1, 5))\n",
        "    data = Dataset.load_from_df(df[['user_id', 'profile_id', 'rating']], reader)\n",
        "    trainset, testset = train_test_split(data, test_size=0.2)\n",
        "    model = SVD()\n",
        "    model.fit(trainset)\n",
        "    predictions = model.test(testset)\n",
        "    print(f\"Model RMSE: {accuracy.rmse(predictions)}\")\n",
        "    return model, data.build_full_trainset()\n",
        "\n",
        "def recommend_profiles(user_id, model, trainset, top_n=5):\n",
        "    \"\"\"Recommends top N profiles for a given user based on learned preferences.\"\"\"\n",
        "    known_items = set(trainset.ur[trainset.to_inner_uid(user_id)])\n",
        "    all_items = set(range(trainset.n_items))\n",
        "    unknown_items = all_items - known_items\n",
        "    predictions = [(iid, model.predict(user_id, iid).est) for iid in unknown_items]\n",
        "    top_profiles = sorted(predictions, key=lambda x: x[1], reverse=True)[:top_n]\n",
        "    return [profile_id for profile_id, _ in top_profiles]\n",
        "\n",
        "# Generate user-profile interactions\n",
        "df_interactions = generate_interactions()\n",
        "\n",
        "# Train the collaborative filtering model\n",
        "model, trainset = build_collaborative_filtering_model(df_interactions)\n",
        "\n",
        "# Recommend profiles for a random user\n",
        "# user_id = random.choice(df_interactions['user_id'].tolist())\n",
        "user_id = 1\n",
        "print(f\"Recommended profiles for User {user_id}: {recommend_profiles(user_id, model, trainset)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDFbaRk-Ax8q",
        "outputId": "949e322b-c232-475b-8a0d-989e38b37eee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 1.4804\n",
            "Model RMSE: 1.4804406786415283\n",
            "Recommended profiles for User 1: [2, 23, 94, 80, 46]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise.model_selection import train_test_split\n",
        "from surprise import accuracy\n",
        "\n",
        "def generate_users(n=100):\n",
        "    data = {\n",
        "        'user_id': list(range(1, n + 1)),\n",
        "        'name': [fake.name() for _ in range(n)],\n",
        "        'age': [random.randint(18, 50) for _ in range(n)],\n",
        "        'gender': [random.choice(['M', 'F', 'Non-binary']) for _ in range(n)],\n",
        "        'location': [fake.city() for _ in range(n)],\n",
        "        'interests': [', '.join(random.sample(\n",
        "            ['music', 'travel', 'sports', 'technology', 'art', 'reading', 'gaming', 'photography', 'cooking', 'fitness'], 3)) for _ in range(n)],\n",
        "        'about_me': [fake.sentence(nb_words=10) for _ in range(n)],\n",
        "        'relationship_goal': [random.choice(['Casual Dating', 'Long-term', 'Friends', 'Networking']) for _ in range(n)],\n",
        "        'personality': [random.choice(['Introvert', 'Extrovert', 'Ambivert']) for _ in range(n)],\n",
        "        'MBTI': [random.choice(['INFJ', 'ENTP', 'ISTJ', 'ENFP', 'ISFJ', 'ESTP', 'INTP', 'ESFJ']) for _ in range(n)],\n",
        "        'likes': [random.sample(range(1, n + 1), random.randint(5, 15)) for _ in range(n)],\n",
        "        'swipes': [random.randint(10, 500) for _ in range(n)],\n",
        "        'messages_sent': [random.randint(0, 100) for _ in range(n)],\n",
        "        'response_rate': [round(random.uniform(0.2, 1.0), 2) for _ in range(n)],\n",
        "    }\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "\n",
        "def generate_interactions(n_users=100, n_items=50):\n",
        "    interactions = []\n",
        "    for user in range(1, n_users + 1):\n",
        "        for _ in range(random.randint(5, 15)):\n",
        "            item = random.randint(1, n_items)\n",
        "            rating = random.choice([1, 2, 3, 4, 5])\n",
        "            interactions.append((user, item, rating))\n",
        "    return pd.DataFrame(interactions, columns=['user_id', 'profile_id', 'rating'])\n",
        "\n",
        "def build_content_matrix(df):\n",
        "    df['content'] = df['interests'] + ' ' + df['personality'] + ' ' + df['about_me'] + ' ' + df['MBTI'] + ' ' + df['location'] + ' ' + df['relationship_goal']\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    content_matrix = vectorizer.fit_transform(df['content'])\n",
        "    return content_matrix, vectorizer\n",
        "\n",
        "def build_cf_model(df):\n",
        "    reader = Reader(rating_scale=(1, 5))\n",
        "    data = Dataset.load_from_df(df[['user_id', 'profile_id', 'rating']], reader)\n",
        "    trainset, _ = train_test_split(data, test_size=0.2)\n",
        "    model = SVD()\n",
        "    model.fit(trainset)\n",
        "    return model, data.build_full_trainset()\n",
        "\n",
        "def update_feedback(user_id, profile_id, rating, df_interactions, cf_model):\n",
        "    \"\"\"Updates user feedback and retrains CF model.\"\"\"\n",
        "    new_entry = pd.DataFrame([[user_id, profile_id, rating]], columns=['user_id', 'profile_id', 'rating'])\n",
        "    df_interactions = pd.concat([df_interactions, new_entry], ignore_index=True)\n",
        "    cf_model, trainset = build_cf_model(df_interactions)\n",
        "    return df_interactions, cf_model, trainset\n",
        "\n",
        "def hybrid_recommend(user_id, df_users, df_interactions, content_matrix, cf_model, trainset, top_n=5):\n",
        "    \"\"\"Combines content-based and collaborative filtering recommendations.\"\"\"\n",
        "    user_idx = df_users[df_users['user_id'] == user_id].index[0]\n",
        "    cb_similarities = cosine_similarity(content_matrix[user_idx], content_matrix).flatten()\n",
        "    cb_ranks = {df_users.iloc[i]['user_id']: cb_similarities[i] for i in range(len(df_users)) if i != user_idx}\n",
        "\n",
        "    known_items = set(trainset.ur[trainset.to_inner_uid(user_id)])\n",
        "    all_items = set(range(trainset.n_items))\n",
        "    unknown_items = all_items - known_items\n",
        "    cf_predictions = {iid: cf_model.predict(user_id, iid).est for iid in unknown_items}\n",
        "\n",
        "    hybrid_scores = {}\n",
        "    for profile_id in set(cb_ranks.keys()).union(cf_predictions.keys()):\n",
        "        cb_score = cb_ranks.get(profile_id, 0)\n",
        "        cf_score = cf_predictions.get(profile_id, 0)\n",
        "        hybrid_scores[profile_id] = 0.5 * cb_score + 0.5 * cf_score\n",
        "\n",
        "    top_profiles = sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
        "    return [profile_id for profile_id, _ in top_profiles]\n",
        "\n",
        "df_users = generate_users(100)\n",
        "df_interactions = generate_interactions()\n",
        "content_matrix, _ = build_content_matrix(df_users)\n",
        "cf_model, trainset = build_cf_model(df_interactions)\n",
        "\n",
        "user_id = random.choice(df_users['user_id'].tolist())\n",
        "print(f\"Hybrid Recommendations for User {user_id}: {hybrid_recommend(user_id, df_users, df_interactions, content_matrix, cf_model, trainset)}\")\n",
        "\n",
        "# Example of feedback integration\n",
        "feedback_user_id = random.choice(df_users['user_id'].tolist())\n",
        "feedback_profile_id = random.choice(df_users['user_id'].tolist())\n",
        "feedback_rating = random.randint(1, 5)\n",
        "df_interactions, cf_model, trainset = update_feedback(feedback_user_id, feedback_profile_id, feedback_rating, df_interactions, cf_model)\n"
      ],
      "metadata": {
        "id": "oPR0R-ItA_Im",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef185285-e613-40f9-f4ff-3f41bba17b27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hybrid Recommendations for User 87: [53, 38, 84, 82, 58]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def generate_interactions(n_users=100, n_items=50):\n",
        "    interactions = []\n",
        "    now = datetime.now()\n",
        "\n",
        "    for user in range(1, n_users + 1):\n",
        "        for _ in range(random.randint(5, 15)):  # Each user interacts with 5-15 profiles\n",
        "            profile = random.randint(1, n_items)\n",
        "            rating = random.choice([1, 2, 3, 4, 5])\n",
        "            timestamp = now - timedelta(days=random.randint(0, 90))  # Last 90 days\n",
        "\n",
        "            skipped = 1 if rating <= 2 else 0  # Assume lower ratings indicate skips\n",
        "            messages = random.randint(0, 10) if rating >= 4 else 0  # More messages for high ratings\n",
        "\n",
        "            interactions.append((user, profile, rating, timestamp, skipped, messages))\n",
        "\n",
        "    return pd.DataFrame(interactions, columns=['user_id', 'profile_id', 'rating', 'timestamp', 'skipped', 'messages'])\n",
        "\n",
        "# Example usage\n",
        "df_interactions = generate_interactions()\n",
        "print(df_interactions.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7piHwd1KxmkT",
        "outputId": "2695a84a-38b0-4b5f-9b01-fdfac70519d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   user_id  profile_id  rating                  timestamp  skipped  messages\n",
            "0        1          13       2 2025-02-26 09:15:11.139001        1         0\n",
            "1        1          23       1 2025-02-04 09:15:11.139001        1         0\n",
            "2        1          47       3 2025-02-28 09:15:11.139001        0         0\n",
            "3        1          48       1 2025-03-08 09:15:11.139001        1         0\n",
            "4        1          38       4 2025-02-21 09:15:11.139001        0         6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# DQN Model\n",
        "def create_embeddings(df_users):\n",
        "    df_users['content'] = df_users['interests'] + ' ' + df_users['personality'] + ' ' + df_users['MBTI']\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    content_matrix = vectorizer.fit_transform(df_users['content']).toarray()\n",
        "    return content_matrix\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "class MatchDQNAgent:\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.model = DQN(state_dim, action_dim)\n",
        "        self.target_model = DQN(state_dim, action_dim)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.memory = deque(maxlen=5000)\n",
        "        self.epsilon = 1.0  # Exploration factor\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995  # Decay rate for epsilon\n",
        "        self.gamma = 0.9    # Discount factor\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, self.action_dim - 1)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                return torch.argmax(self.model(torch.FloatTensor(state))).item()\n",
        "\n",
        "    def train(self, batch_size=32):\n",
        "        if len(self.memory) < batch_size:\n",
        "            return\n",
        "\n",
        "        batch = random.sample(self.memory, batch_size)\n",
        "        states, actions, rewards, next_states = zip(*batch)\n",
        "\n",
        "        states = torch.FloatTensor(states)\n",
        "        actions = torch.LongTensor(actions)\n",
        "        rewards = torch.FloatTensor(rewards)\n",
        "        next_states = torch.FloatTensor(next_states)\n",
        "\n",
        "        q_values = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "        next_q_values = self.target_model(next_states).max(1)[0].detach()\n",
        "        target_q_values = rewards + self.gamma * next_q_values\n",
        "\n",
        "        loss = self.criterion(q_values, target_q_values)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Decay epsilon\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def remember(self, state, action, reward, next_state):\n",
        "        self.memory.append((state, action, reward, next_state))\n",
        "\n",
        "# Reward function update\n",
        "import numpy as np\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def get_reward(df_interactions, user_id, profile_id, decay_factor=0.99):\n",
        "    interactions = df_interactions[(df_interactions['user_id'] == user_id) &\n",
        "                                   (df_interactions['profile_id'] == profile_id)].copy()  # Explicit copy to avoid warning\n",
        "\n",
        "    if interactions.empty:\n",
        "        return 0  # No prior interaction, neutral reward\n",
        "\n",
        "    # Time decay: newer interactions contribute more\n",
        "    max_timestamp = df_interactions['timestamp'].max()\n",
        "    interactions['time_weight'] = np.exp(-decay_factor * (max_timestamp - interactions['timestamp']).dt.days)\n",
        "\n",
        "    # Base reward: Sum of ratings (assuming a 1-5 scale)\n",
        "    reward = (interactions['rating'] * interactions['time_weight']).sum()\n",
        "\n",
        "    # Bonus for strong interest (ratings >= 4)\n",
        "    reward += (2 * (interactions['rating'] >= 4) * interactions['time_weight']).sum()\n",
        "\n",
        "    # Penalize skips\n",
        "    reward -= (3 * (interactions['skipped'] == 1) * interactions['time_weight']).sum()\n",
        "\n",
        "    # Reward for conversations\n",
        "    reward += (5 * interactions['messages'] * interactions['time_weight']).sum()\n",
        "\n",
        "    return reward\n",
        "\n",
        "\n",
        "\n",
        "# State representation improvement\n",
        "def build_state(user_embedding, user_id, df_interactions):\n",
        "    past_matches = df_interactions[df_interactions['user_id'] == user_id]['profile_id'].values\n",
        "    match_count = len(past_matches)\n",
        "    engagement_score = df_interactions[df_interactions['user_id'] == user_id]['rating'].mean() if match_count > 0 else 0\n",
        "\n",
        "    return np.append(user_embedding, [match_count, engagement_score])\n",
        "\n",
        "\n",
        "# Training the RL agent\n",
        "def run_dqn_training(df_users, df_interactions, num_episodes=1000):\n",
        "    embeddings = create_embeddings(df_users)\n",
        "    state_dim = embeddings.shape[1] + 2  # Adding past match count & engagement score\n",
        "    agent = MatchDQNAgent(state_dim=state_dim, action_dim=len(df_users))\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        user_idx = random.choice(range(len(df_users)))\n",
        "        user_embedding = embeddings[user_idx]\n",
        "        user_state = build_state(user_embedding, user_idx+1, df_interactions)\n",
        "\n",
        "        profile_idx = agent.select_action(user_state)\n",
        "        profile_embedding = embeddings[profile_idx]\n",
        "\n",
        "        reward = get_reward(df_interactions, user_idx+1, profile_idx+1)\n",
        "        next_state = build_state(user_embedding, user_idx+1, df_interactions)  # Assuming user state updates gradually\n",
        "\n",
        "        agent.remember(user_state, profile_idx, reward, next_state)\n",
        "        agent.train()\n",
        "\n",
        "    return agent\n",
        "\n",
        "# Generate user data (for testing)\n",
        "df_users = generate_users(100)\n",
        "df_interactions = generate_interactions()\n",
        "\n",
        "# Initialize hybrid model and content-based recommendations\n",
        "content_matrix = create_embeddings(df_users)\n",
        "\n",
        "# Train RL agent using interactions\n",
        "agent = run_dqn_training(df_users, df_interactions, num_episodes=1000)\n",
        "\n",
        "# Example of generating recommendations for a user\n",
        "user_id = random.choice(df_users['user_id'].tolist())\n",
        "\n",
        "# RL Agent recommendation\n",
        "user_embedding = content_matrix[user_id-1]  # Example user embedding\n",
        "user_state = build_state(user_embedding, user_id, df_interactions)\n",
        "profile_idx = agent.select_action(user_state)\n",
        "print(f\"RL Agent Recommendation for User {user_id}: {df_users.iloc[profile_idx]['name']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ew_kDSE6W7fU",
        "outputId": "9ee35a48-7704-422a-8e90-fcda095460fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RL Agent Recommendation for User 88: Shawn Mejia\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import models, transforms\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "resnet = models.resnet50(pretrained=True)\n",
        "model = torch.nn.Sequential(*list(resnet.children())[:-1])  # remove FC\n",
        "model.eval()\n",
        "\n",
        "def get_embedding(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image_tensor = transform(image).unsqueeze(0)  # shape (1, 3, 224, 224)\n",
        "    with torch.no_grad():\n",
        "        embedding = model(image_tensor).squeeze().numpy()  # shape (2048,)\n",
        "    return embedding / np.linalg.norm(embedding)  # normalize\n",
        "\n",
        "def extract_all_embeddings(image_dir):\n",
        "    image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir)\n",
        "                   if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
        "    embeddings = [get_embedding(path) for path in image_paths]\n",
        "    return image_paths, np.array(embeddings)\n",
        "\n",
        "def recommend_similar_images(target_index, image_paths, embeddings, top_n=5):\n",
        "    target_emb = embeddings[target_index].reshape(1, -1)\n",
        "    sims = cosine_similarity(target_emb, embeddings)[0]\n",
        "    sims[target_index] = -1  # exclude self\n",
        "    top_indices = sims.argsort()[::-1][:top_n]\n",
        "    return [image_paths[i] for i in top_indices]\n",
        "\n",
        "image_dir = \"profile_images\"\n",
        "image_paths, embeddings = extract_all_embeddings(image_dir)\n",
        "\n",
        "target_index = 0\n",
        "recommendations = recommend_similar_images(target_index, image_paths, embeddings)\n",
        "\n",
        "print(\"Top similar profiles for:\", image_paths[target_index])\n",
        "for rec in recommendations:\n",
        "    print(\"→\", rec)\n"
      ],
      "metadata": {
        "id": "mjbSzQbPrCgC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "outputId": "7acdb1da-eb5d-42a2-c09b-091b767198c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 104MB/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'profile_images'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c7cb20616824>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mimage_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"profile_images\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mimage_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_all_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mtarget_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-c7cb20616824>\u001b[0m in \u001b[0;36mextract_all_embeddings\u001b[0;34m(image_dir)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_all_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir)\n\u001b[0m\u001b[1;32m     28\u001b[0m                    if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n\u001b[1;32m     29\u001b[0m     \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_paths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'profile_images'"
          ]
        }
      ]
    }
  ]
}